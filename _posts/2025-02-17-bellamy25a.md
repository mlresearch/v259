---
title: 'Labrador: Exploring the limits of masked language modeling for laboratory
  data'
abstract: In this work we introduce Labrador, a pre-trained Transformer model for
  laboratory data. Labrador and BERT were pre-trained on a corpus of 100 million lab
  test results from electronic health records (EHRs) and evaluated on various downstream
  outcome prediction tasks. Both models demonstrate mastery of the pre-training task
  but neither consistently outperform XGBoost on downstream supervised tasks. Our
  ablation studies reveal that transfer learning shows limited effectiveness for BERT
  and achieves marginal success with Labrador. We explore the reasons for the failure
  of transfer learning and suggest that the data generating process underlying each
  patient cannot be characterized sufficiently using labs alone, among other factors.
  We encourage future work to focus on joint modeling of multiple EHR data categories
  and to include tree-based baselines in their evaluations.
software: https://github.com/DavidBellamy/labrador
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bellamy25a
month: 0
tex_title: 'Labrador: Exploring the limits of masked language modeling for laboratory
  data'
firstpage: 104
lastpage: 129
page: 104-129
order: 104
cycles: false
bibtex_author: Bellamy, David and Kumar, Bhawesh and Wang, Cindy and Beam, Andrew
author:
- given: David
  family: Bellamy
- given: Bhawesh
  family: Kumar
- given: Cindy
  family: Wang
- given: Andrew
  family: Beam
date: 2025-02-17
address:
container-title: Proceedings of the 4th Machine Learning for Health Symposium
volume: '259'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v259/main/assets/bellamy25a/bellamy25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
