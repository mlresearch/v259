---
title: Are Clinical T5 Models Better for Clinical Text?
abstract: Large language models with a transformer-based encoder/decoder architecture,
  such as T5, have become standard platforms for supervised tasks. To bring these
  technologies to the clinical domain, recent work has trained new or adapted existing
  models to clinical data. However, the evaluation of these clinical T5 models and
  comparison to other models has been limited. Are the clinical T5 models better choices
  than FLAN-tuned generic T5 models? Do they generalize better to new clinical domains
  that differ from the training sets? We comprehensively evaluate these models across
  several clinical tasks and domains. We find that clinical T5 models provide marginal
  improvements over existing models, and perform worse when evaluated on different
  domains. Our results inform future choices in developing clinical LLMs.
software: https://github.com/yli-z/ml4h_are_clinical_t5_models_better_for_clinical_text
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li25a
month: 0
tex_title: Are Clinical T5 Models Better for Clinical Text?
firstpage: 636
lastpage: 667
page: 636-667
order: 636
cycles: false
bibtex_author: Li, Yahan and Harrigian, Keith and Zirikly, Ayah and Dredze, Mark
author:
- given: Yahan
  family: Li
- given: Keith
  family: Harrigian
- given: Ayah
  family: Zirikly
- given: Mark
  family: Dredze
date: 2025-02-17
address:
container-title: Proceedings of the 4th Machine Learning for Health Symposium
volume: '259'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v259/main/assets/li25a/li25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
