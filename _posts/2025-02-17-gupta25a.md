---
title: Multi-Modal Self-Supervised Learning for Surgical Feedback Effectiveness Assessment
abstract: During surgical training, real-time feedback from trainers to trainees is
  important for preventing errors and enhancing long-term skill acquisition. Accurately
  predicting the effectiveness of this feedback, specifically whether it leads to
  a change in trainee behavior, is crucial for developing methods for improving surgical
  training and education. However, relying on human annotations to assess feedback
  effectiveness is laborious and prone to biases, underscoring the need for an automated,
  scalable, and objective method. Creating such an automated system poses challenges,
  as it requires an understanding of both the verbal feedback delivered by the trainer
  and the visual context of the real-time surgical scene. To address this, we propose
  a method that integrates information from transcribed verbal feedback and corresponding
  surgical video to predict feedback effectiveness. Our findings show that both transcribed
  feedback and surgical video are individually predictive of trainee behavior changes,
  and their combination achieves an AUROC of 0.70 Â± 0.02, improving prediction accuracy
  by up to 6.6%. Additionally, we introduce self-supervised fine-tuning as a strategy
  for enhancing surgical video representation learning, which is scalable and further
  enhances prediction performance. Our results demonstrate the potential of multi-modal
  learning to advance the automated assessment of surgical feedback.
software: https://github.com/arushig100/Multi-Modal-SSL-for-Surgical-Feedback-Effectiveness-Assessment
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gupta25a
month: 0
tex_title: Multi-Modal Self-Supervised Learning for Surgical Feedback Effectiveness
  Assessment
firstpage: 440
lastpage: 455
page: 440-455
order: 440
cycles: false
bibtex_author: Gupta, Arushi and Kocielnik, Rafal Dariusz and Wang, Jiayun and Nasriddinov,
  Firdavs and Yang, Cherine and Wong, Elyssa and Anandkumar, Anima and Hung, Andrew
author:
- given: Arushi
  family: Gupta
- given: Rafal Dariusz
  family: Kocielnik
- given: Jiayun
  family: Wang
- given: Firdavs
  family: Nasriddinov
- given: Cherine
  family: Yang
- given: Elyssa
  family: Wong
- given: Anima
  family: Anandkumar
- given: Andrew
  family: Hung
date: 2025-02-17
address:
container-title: Proceedings of the 4th Machine Learning for Health Symposium
volume: '259'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v259/main/assets/gupta25a/gupta25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
