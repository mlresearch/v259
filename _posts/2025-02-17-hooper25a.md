---
title: A Study on Context Length and Efficient Transformers for Biomedical Image Analysis
abstract: Biomedical images are often high-resolution and multi-dimensional, presenting
  computational challenges for deep neural networks. These computational challenges
  are compounded when training transformers due to the self-attention operator, which
  scales quadratically with context length. Recent works have proposed alternatives
  to self-attention that scale more favorably with context length, alleviating these
  computational difficulties and potentially enabling more efficient application of
  transformers to large biomedical images. However, a systematic evaluation on this
  topic is lacking. In this study, we investigate the impact of context length on
  biomedical image analysis and we evaluate the performance of recently proposed substitutes
  for self-attention. We first curate a suite of biomedical imaging datasets, including
  2D and 3D data for segmentation, denoising, and classification tasks. We then analyze
  the impact of context length on network performance using the Vision Transformer
  and Swin Transformer. Our findings reveal a strong relationship between context
  length and performance, particularly for pixel-level prediction tasks. Finally,
  we show that recent attention-free models demonstrate significant improvements in
  efficiency while maintaining comparable performance to self-attention-based models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hooper25a
month: 0
tex_title: A Study on Context Length and Efficient Transformers for Biomedical Image
  Analysis
firstpage: 471
lastpage: 489
page: 471-489
order: 471
cycles: false
bibtex_author: Hooper, Sarah M. and Xue, Hui
author:
- given: Sarah M.
  family: Hooper
- given: Hui
  family: Xue
date: 2025-02-17
address:
container-title: Proceedings of the 4th Machine Learning for Health Symposium
volume: '259'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 2
  - 17
pdf: https://raw.githubusercontent.com/mlresearch/v259/main/assets/hooper25a/hooper25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
